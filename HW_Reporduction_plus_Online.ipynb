{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcTLg4EPTKWdkNfmYyYTAl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kslote1/HW-SIAM/blob/main/HW_Reporduction_plus_Online.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hui-Walter\n",
        "\n",
        "Below is a reproduction of the HW paper results using MLE estimates."
      ],
      "metadata": {
        "id": "sX_AS8B-I_s0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the likelihood function\n",
        "def likelihood(params, data):\n",
        "    o1, o2, a1, a2, b1, b2 = params\n",
        "    X1, Y1, Z1, W1, X2, Y2, Z2, W2 = data\n",
        "\n",
        "    tot1 = X1 + Y1 + Z1 + W1\n",
        "    tot2 =  X2 + Y2 + Z2 + W2\n",
        "\n",
        "    p1 = (o1 * (1-a1) * (1-b1) + (1-o1) * a1 * b1)\n",
        "    p2 = (o1 * (1-a1) * b1 + (1-o1) * a1 * (1-b1))\n",
        "    p3 = (o1 * a1 * (1-b1) + (1-o1) * (1-a1) * b1)\n",
        "    p4 = (o1 * a1 * b1 + (1-o1) * (1-a1) * (1-b1))\n",
        "    p5 = (o2 * (1-a2) * (1-b2) + (1-o2) * a2 * b2)\n",
        "    p6 = (o2 * (1-a2) * b2 + (1-o2) * a2 * (1-b2))\n",
        "    p7 = (o2 * a2 * (1-b2) + (1-o2) * (1-a2) * b2)\n",
        "    p8 = (o2 * a2 * b2 + (1-o2) * (1-a2) * (1-b2))\n",
        "    eps = 1e-10\n",
        "    L = (\n",
        "        (p1 + eps)**(X1/tot1) *\n",
        "        (p2 + eps)**(Y1/tot1) *\n",
        "        (p3 + eps)**(Z1/tot1) *\n",
        "        (p4 + eps)**(W1/tot1) *\n",
        "        (p5 + eps)**(X2/tot2) *\n",
        "        (p6 + eps)**(Y2/tot2) *\n",
        "        (p7 + eps)**(Z2/tot2) *\n",
        "        (p8 + eps)**(W2/tot2)\n",
        "    )\n",
        "\n",
        "    return -np.log(L) # Return the negative log-likelihood for minimization\n",
        "\n",
        "\n",
        "# Observed data: X1, Y1, Z1, W1, X2, Y2, Z2, W2\n",
        "#data = (25, 30, 35, 60, 45, 50, 55, 80)\n",
        "data = (14, 4, 9, 528, 887, 21, 37, 367)\n",
        "\n",
        "# Initial parameter guesses: o1, o2, a1, a2, b1, b2\n",
        "init_params = [0.5, 0.5, 0.1, 0.1, 0.1, 0.1]\n",
        "\n",
        "# Parameter bounds\n",
        "bounds = [(0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]\n",
        "\n",
        "# Minimize the negative log-likelihood\n",
        "result = minimize(likelihood, init_params, args=(data,), bounds=bounds)\n",
        "\n",
        "# Get the maximum likelihood estimates for the parameters\n",
        "o1_mle, o2_mle, a1_mle, a2_mle, b1_mle, b2_mle = result.x\n",
        "\n",
        "print(\"Maximum Likelihood Estimates:\")\n",
        "print(f\"o1: {o1_mle:.4f}, o2: {o2_mle:.4f}, a1: {a1_mle:.4f}, a2: {a2_mle:.4f}, b1: {b1_mle:.4f}, b2: {b2_mle:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oefcH26DIzYr",
        "outputId": "65321bd0-50f1-4848-96eb-a4a3208ec0fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Likelihood Estimates:\n",
            "o1: 0.0257, o2: 0.7075, a1: 0.0071, a2: 0.0371, b1: 0.0166, b2: 0.0077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Online Version\n",
        "\n",
        "Below is an online version that uses MLE esitmate again.\n",
        "It seems to work.\n",
        "\n",
        "Plan is to change the estimator, expand to n-classes and n-populations."
      ],
      "metadata": {
        "id": "-LQhSyZPJPT1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kh30Qi2Iuar"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import minimize\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class OnlineEMHuiWalter:\n",
        "    def __init__(self, params, step_size=0.1):\n",
        "        self.params = params\n",
        "        self.step_size = step_size\n",
        "        self.alpha1 = []\n",
        "        self.alpha2 = []\n",
        "        self.beta1 = []\n",
        "        self.beta2 = []\n",
        "        self.theta1 = []\n",
        "        self.theta2 = []\n",
        "\n",
        "\n",
        "    def calculate_probabilities(self, alpha1, alpha2, beta1, beta2, theta1, theta2):\n",
        "        # Calculate probabilities for each cell in the first population\n",
        "        p1 = theta1 * (1 - beta1) * (1 - beta2) + (1 - theta1) * alpha1 * alpha2\n",
        "        p2 = theta1 * (1 - beta1) * beta2 + (1 - theta1) * alpha1 * (1 - alpha2)\n",
        "        p3 = theta1 * beta1 * (1 - beta2) + (1 - theta1) * (1 - alpha1) * alpha2\n",
        "        p4 = theta1 * beta1 * beta2 + (1 - theta1) * (1 - alpha1) * (1 - alpha2)\n",
        "\n",
        "        # Calculate probabilities for each cell in the second population\n",
        "        p5 = theta2 * (1 - beta1) * (1 - beta2) + (1 - theta2) * alpha1 * alpha2\n",
        "        p6 = theta2 * (1 - beta1) * beta2 + (1 - theta2) * alpha1 * (1 - alpha2)\n",
        "        p7 = theta2 * beta1 * (1 - beta2) + (1 - theta2) * (1 - alpha1) * alpha2\n",
        "        p8 = theta2 * beta1 * beta2 + (1 - theta2) * (1 - alpha1) * (1 - alpha2)\n",
        "\n",
        "        return p1, p2, p3, p4, p5, p6, p7, p8\n",
        "\n",
        "    def negative_log_likelihood(self, params, pop_one_data, pop_two_data):\n",
        "        # Unpack the parameters\n",
        "        alpha1, alpha2, beta1, beta2, theta1, theta2 = params\n",
        "        p1, p2, p3, p4, p5, p6, p7, p8 = self.calculate_probabilities(alpha1, alpha2, beta1, beta2, theta1, theta2)\n",
        "\n",
        "        X1, X2, X3, X4, tot1 = self.table_counts(pop_one_data)\n",
        "        X5, X6, X7, X8, tot2 = self.table_counts(pop_two_data)\n",
        "\n",
        "        eps = 1e-10\n",
        "        L = (\n",
        "            (p1 + eps)**(X1/tot1) *\n",
        "            (p2 + eps)**(X2/tot1) *\n",
        "            (p3 + eps)**(X3/tot1) *\n",
        "            (p4 + eps)**(X4/tot1) *\n",
        "            (p5 + eps)**(X5/tot2) *\n",
        "            (p6 + eps)**(X6/tot2) *\n",
        "            (p7 + eps)**(X7/tot2) *\n",
        "            (p8 + eps)**(X8/tot2)\n",
        "        )\n",
        "\n",
        "        return -np.log(L) # Return the negative log-likelihood for minimization\n",
        "\n",
        "    def mle(self, pop_one_data, pop_two_data):\n",
        "\n",
        "        # Initial guess\n",
        "        initial_guess = self.params #[0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
        "\n",
        "        bounds = [(0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]\n",
        "\n",
        "        # Perform minimization\n",
        "        result = minimize(self.negative_log_likelihood, initial_guess, args=(pop_one_data, pop_two_data,), bounds=bounds)\n",
        "\n",
        "        return result.x\n",
        "\n",
        "    def table_counts(self, data):\n",
        "        X1 = np.sum((data[:, 0] == 1) & (data[:, 1] == 1))  # Both classifiers predict 1\n",
        "        X2 = np.sum((data[:, 0] == 1) & (data[:, 1] == 0))  # First predicts 1, second predicts 0\n",
        "        X3 = np.sum((data[:, 0] == 0) & (data[:, 1] == 1))  # First predicts 0, second predicts 1\n",
        "        X4 = np.sum((data[:, 0] == 0) & (data[:, 1] == 0))  # Both classifiers predict 0\n",
        "\n",
        "        tot1 = X1 + X2 + X3 + X4  # Total counts\n",
        "        return X1, X2, X3, X4, tot1\n",
        "\n",
        "    def update(self, pop_one_data, pop_two_data):\n",
        "\n",
        "         # Unpack the parameters\n",
        "        alpha1, alpha2, beta1, beta2, theta1, theta2 = self.params\n",
        "\n",
        "        a1, a2, b1, b2, t1, t2 = self.mle(pop_one_data, pop_two_data)\n",
        "\n",
        "        self.alpha1.append(a1)\n",
        "        self.alpha2.append(a2)\n",
        "        self.beta1.append(b1)\n",
        "        self.beta2.append(b2)\n",
        "        self.theta1.append(t1)\n",
        "        self.theta2.append(t2)\n",
        "\n",
        "        alpha1 =  np.mean(self.alpha1)\n",
        "        alpha2 = np.mean(self.alpha2)\n",
        "        beta1  = np.mean(self.beta1)\n",
        "        beta2  = np.mean(self.beta2)\n",
        "        theta1 = np.mean(self.theta1)\n",
        "        theta2 = np.mean(self.theta2)\n",
        "\n",
        "        # Ensure parameters remain within valid range\n",
        "        self.params = [\n",
        "            max(0, min(1, alpha1)), max(0, min(1, alpha2)),\n",
        "            max(0, min(1, beta1)), max(0, min(1, beta2)),\n",
        "            max(0, min(1, theta1)), max(0, min(1, theta2))\n",
        "        ]\n",
        "\n",
        "\n",
        "def generate_data(n_samples, n_classes, prior, se, sp):\n",
        "    n_tests = len(se)\n",
        "\n",
        "    data = np.zeros((n_samples, n_tests))\n",
        "    true_classes = np.random.choice(n_classes, n_samples, p=[prior, 1 - prior])\n",
        "\n",
        "    for i, c in enumerate(true_classes):\n",
        "        for j in range(n_tests):\n",
        "            if c == 1:\n",
        "                data[i, j] = np.random.choice([0, 1], p=[1-se[j], se[j]])\n",
        "            else:\n",
        "                data[i, j] = np.random.choice([0, 1], p=[sp[j], 1-sp[j]])\n",
        "\n",
        "    return data\n",
        "\n",
        "# True parameters for generating synthetic data\n",
        "true_priors = [0.6, 0.4]\n",
        "true_se = [0.9, 0.7]\n",
        "true_sp = [0.8, 0.6]\n",
        "n_classes = 2\n",
        "\n",
        "# Generate synthetic data\n",
        "n_samples = 100000\n",
        "pop_one_data = generate_data(n_samples, n_classes, true_priors[0], true_se, true_sp)\n",
        "pop_two_data = generate_data(n_samples, n_classes, true_priors[1], true_se, true_sp)\n",
        "\n",
        "# Initialize the OnlineEMHuiWalter class\n",
        "init_params = [0.6, 0.4, 0.9, 0.7, 0.8, 0.6]\n",
        "step_size = 0.001\n",
        "em = OnlineEMHuiWalter(init_params, step_size)\n",
        "for i in range(n_samples):\n",
        "    em.update(pop_one_data[:i], pop_two_data[:i])\n",
        "\n",
        "print(em.params)"
      ]
    }
  ]
}